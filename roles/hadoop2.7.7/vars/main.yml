---
hadoop:
 filename: hadoop-2.7.7.tar.gz
 remote_dir: /tmp/
 install_dir: /home/hadoop/hadoop
 env_file: /home/hadoop/.bash_profile
 journalnode_dir: /data/hadoop/journalnode/data/
 tmp_dir: /data/hadoop/hadoop/
 pid_dir: /home/hadoop/hadoop/pid/
 log_dir: /data/hadoop/log/

 
########## hdfs-site.xml #################
# python -c 'import math ; print int(math.log(N) * 20)'  
# N 集群服务器数量
# default 10
dfs_namenode_handler_count: 100
# default 10
dfs_datanode_handler_count: 100

# default file://${hadoop.tmp.dir}/dfs/name
# 多个目录,逗号分隔
dfs_namenode_name_dir: /data/hadoop/hadoop/dfs/name/

# default file://${hadoop.tmp.dir}/dfs/data
# 多个目录,逗号分隔
dfs_datanode_data_dir: /data/hadoop/hadoop/dfs/data/ 

# 使用df -h 显示：总磁盘大小2000G，使用1930G，剩余40G，就会发现：2000G -（1930G+40G）= 30G，还差了30G空间。
# 就是因为这30G空间的问题导致你磁盘写满，如果你配置datanode hdfs-site.xml里的dfs.datanode.du.reserved小于30G的话，而我们设置的是20G，所以磁盘就被写满 了，预留空间就没有起到实际作用
# hadoop dfs.datanode.du.reserved的值 = 总磁盘大小 - （使用的空间 + 剩余空间 ）+ 设置的预留空间
# 比如： 
# 在上面的基础上你设置预留空间为20G，那么dfs.datanode.du.reserved就可以设置为： 
# 2000G -（1930G + 40G） + 20G = 50G
# 80G: 80 * 1024 * 1024 * 1024 = 85899345920
dfs_datanode_du_reserved: 85899345920
# 副本数量
dfs_replication: 3

# Specifies the maximum amount of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second.
# default 1048576 bytes
# 1024 * 1024 = 1M/s
# 30 * 1024 * 1024 = 30 M/s
dfs_datanode_balance_bandwidthPerSec: 31457280

# Specifies the maximum number of threads to use for transferring data in and out of the DN.
# default 4096
# datanode上负责进行文件操作的线程数。如果需要处理的文件过多，而这个参数设置得过低就会有一部分文件
# 处理不过来,如果这样的线程过多，系统内存就会暴掉.
dfs_datanode_max_transfer_threads: 4096

########## core-site.xml #################
# Indicates the length of the listen queue for servers accepting client connections.
# default 128
ipc_server_listen_queue_size: 1024


########## yarn-site.xml #################
# default ${hadoop.tmp.dir}/nm-local-dir
# 存放application执行本地文件的根目录，执行完毕后删除，按用户名存储
yarn_nodemanager_local_dirs: /data/hadoop/hadoop/nm-local-dir
# default ${yarn.log.dir}/userlogs
# 存放application本地执行日志的根目录，执行完毕后删除，按用户名存储
yarn_nodemanager_log_dirs: /data/hadoop/yarn/log/userlogs
# 日志的保留时间,log aggregation没有enable时，有效
yarn_nodemanager_log_retain_second: 604800  

# 历史日志访问
yarn_log_server_url: http://node2:19888/jobhistory/logs
# 是否开启日志聚合功能
yarn_log_aggregation_enable: "true"
# 收集的日志的保留时间，以秒为单位，到时后被删除，保留30天后删除
yarn_log_aggregation_retain_seconds: 2592000
# log server的地址 http://hostname:19888/jobhistory/logs
#yarn.log.server.url: 
# /app-logs  聚合日志后在hdfs的存放地址
yarn_nodemanager_remote_app_log_dir: /app-log
# 集合日志后的存放地址由 ${remote-app-log-dir}/${user}/{thisParam}构成
yarn_nodemanager_remote_app_log_dir_suffix: logs
#  application执行结束后延迟10min删除本地文件及日志
yarn_nodemanager_delete_debug_delay_sec: 86400
# 节点上YARN可使用的物理内存总量，默认是8192（MB）
yarn_nodemanager_resource_memory_mb: 60000
# NodeManager总的可用虚拟CPU个数，默认是8
yarn_nodemanager_resource_cpu_vcores: 8
# 单个任务可申请的最少物理内存量，默认是1024（MB）
yarn_scheduler_minimum_allocation_mb: 2048
# 单个任务可申请的最多物理内存量，默认是8192（MB）
yarn_scheduler_maximum_allocation_mb: 20480
# 单个任务可申请的最小虚拟CPU个数，默认是1
yarn_scheduler_minimum_allocation_vcores: 2
# 单个任务可申请的最大虚拟CPU个数，默认是4
yarn_scheduler_maximum_allocation_vcores: 20
# 是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true
yarn_nodemanager_pmem_check_enabled: true
# 是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true
yarn_nodemanager_vmem_check_enabled: true
# 处理来自NodeManager的RPC请求的Handler数目，默认是50
yarn_resourcemanager_resource_tracker_client_thread_count: 100
# 处理来自ApplicationMaster的RPC请求的Handler数目，默认是50
yarn_resourcemanager_scheduler_client_thread_count: 100

########## mapred-site.xml #################
# default /tmp/hadoop-yarn/staging
# 存在hdfs目录
yarn_app_mapreduce_am_staging_dir: /mapjob/hadoop_yarn/staging
# default ${yarn.app.mapreduce.am.staging-dir}/history/done
# 存在hdfs目录
mapreduce_jobhistory_done_dir: /mapjob/history/done
# default ${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate
# 存在hdfs目录
mapreduce_jobhistory_intermediate_done_dir: /mapjob/history/done_intermediate
# MapReduce JobHistory Server IPC host:port
mapreduce_jobhistory_address: 0.0.0.0:10020
# MapReduce JobHistory Server Web UI host:port
mapreduce_jobhistory_webapp_address: 0.0.0.0:19888

########## yarn-env.sh #################
# default $HADOOP_YARN_HOME/logs
yarn_log_dir: /data/hadoop/yarn/log
# default /tmp yarn-hadoop-nodemanager.pid yarn-hadoop-resourcemanager.pid
yarn_pid_dir: /home/hadoop/hadoop/pid/

######### mapred-env.sh #################
hadoop_mapred_log_dir: /data/hadoop/mapred_log
hadoop_mapred_pid_dir: /home/hadoop/hadoop/pid

hadoop_slaves:
 - node2
 - node3
 - node4

JAVA_HOME: /opt/jdk/
